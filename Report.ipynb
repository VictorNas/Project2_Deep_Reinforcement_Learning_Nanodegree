{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dad21cf0",
   "metadata": {},
   "source": [
    "### 1. The Enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca427d7",
   "metadata": {},
   "source": [
    "In this project there is a implementation of the [Deep Deterministic Policy Gradient (DDPG)](https://arxiv.org/pdf/1509.02971.pdf) to solve the [Reacher Enviroment](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher) with multiple agents. In this environment, a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible. In this Enviroment with multiple agents, at each episode each agent receiveis its own score. Therefore, the score of the episode s the mean os the scores of the agents. The enviroment is considered solved when the agent achives a mean of +30 over 100 consecutives episodes.\n",
    "\n",
    "All the implemantation is avaliable in the [Training Notebook](Continous_Control_Training.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f592df44",
   "metadata": {},
   "source": [
    "### 2. Learning Algorithm DDPG: Deep Deterministic Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e865773d",
   "metadata": {},
   "source": [
    "#### The Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2db7be",
   "metadata": {},
   "source": [
    "DDPG was introduced as an Actor-Critic Method, but some researchers think DDPG is best classified as a DQN method for continuous action spaces. In this algorithm there are two networks the **Actor** and the ***Critic**.\n",
    "The **Actor** will try to determine the best believe action for any given state.\n",
    "The **Critic** will try to evaluate the action value function for a given state using the best belive action given by the **actor** .\n",
    "\n",
    "Similar to DQN, the DDPG also uses a **Buffer Replay**, **Targets Networks qith soft updates** (one for the critic and one for the actor) reduce the instabilities in the network during the training.\n",
    "\n",
    "For the explorarion Noise process the autors used the **Ornstein-Uhlenbeck process**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8524243",
   "metadata": {},
   "source": [
    "#### Pseudocode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b42e28",
   "metadata": {},
   "source": [
    "![alt text](assets/ddpgpcode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caed0e1",
   "metadata": {},
   "source": [
    "Image from:  [Deep Deterministic Policy Gradient (DDPG) paper](https://arxiv.org/pdf/1509.02971.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a250a9ca",
   "metadata": {},
   "source": [
    "#### Adaptation to solve the Reacher Enviroment with 20 Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c342f6bc",
   "metadata": {},
   "source": [
    "The code implementation in the project is an adaptation of the code used to solve the [bipedal environment](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-bipedal) from Udacity Github repository.\n",
    "\n",
    "Since we are training the 20 agents at the same time and all the agents have the same action and state structure, only one critic and actor network was trained to control all the 20 agents. Only one replay Buffer was used and this buffer has experiences off all the agents. At each learn step a batch of experiences was sampled from the replay buffer and the networks were updated with the experiences of all agents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f1b9b0",
   "metadata": {},
   "source": [
    "#### Actor Archtecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e573c2e4",
   "metadata": {},
   "source": [
    "The **Actor** Network has the following archtecture"
   ]
  },
  {
   "cell_type": "raw",
   "id": "897a8fb0",
   "metadata": {},
   "source": [
    "Actor(\n",
    "  (fc1): Linear(in_features=33, out_features=128, bias=True)\n",
    "  (fc2): Linear(in_features=128, out_features=256, bias=True)\n",
    "  (fc3): Linear(in_features=256, out_features=4, bias=True)\n",
    "  (batch_fc1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb3d8a1",
   "metadata": {},
   "source": [
    "* The First linear layer **fc1** is follewed by a BatchNorm Layer  **batch_fc1** and a **LeakyRelu** activation function.\n",
    "\n",
    "* The Second linear layer **fc2** is follewed by **LeakyRelu** activation function.\n",
    "\n",
    "* The Third linear layer **fc3** is follewed by **tanh** activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e7e8a5",
   "metadata": {},
   "source": [
    "#### Critic Archtecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40897c3a",
   "metadata": {},
   "source": [
    "The **Critic** Network has the following archtecture"
   ]
  },
  {
   "cell_type": "raw",
   "id": "18c36455",
   "metadata": {},
   "source": [
    "Critic(\n",
    "  (fcs1): Linear(in_features=33, out_features=128, bias=True)\n",
    "  (fc2): Linear(in_features=132, out_features=256, bias=True)\n",
    "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
    "  (batch_fc1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeb1379",
   "metadata": {},
   "source": [
    "* The First linear layer **fc1** is follewed by a BatchNorm Layer  **batch_fc1** and a **LeakyRelu** activation function.\n",
    "\n",
    "* The Second linear layer **fc2** is follewed by **LeakyRelu** activation function.\n",
    "\n",
    "* The Third linear layer **fc3** is not follewed by any activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6e6b29",
   "metadata": {},
   "source": [
    "#### Chosen Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae9516",
   "metadata": {},
   "source": [
    "After some experimentation the following hyperparameters were choosen:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7803b2",
   "metadata": {},
   "source": [
    "| hyperparameter | Value | Descrption\n",
    "| --- | --- | --- |\n",
    "| BUFFER_SIZE | 1e6|replay buffer size|\n",
    "| BATCH_SIZE | 128|minibatch size|\n",
    "| GAMMA | 0.99 |discount factor|\n",
    "| TAU | 1e-3|for soft update of target parameters|\n",
    "|LR_ACTOR| 3e-4|learning rate of the actor |\n",
    "|LR_CRITIC|1e-3 |learning rate of the critic|\n",
    "|UPDATE_STEPS|10 |update every steps|\n",
    "|NUMBER_UPDATES|8|number of times we update the network after each UPDATE_STEPS|\n",
    "|OPTMIZER_ACTOR|ADAM|Optimizr of the actor|\n",
    "|OPTMIZER_CRITIC|ADAM|Optimizr of the critic|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bd390f",
   "metadata": {},
   "source": [
    "### 2.Plot of Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d45da7a",
   "metadata": {},
   "source": [
    "The enviroment is considered solved when the agent achives a mean of +30 over 100 consecutives episodes.\n",
    "The Agent was able to solve the enviroment at first in episode 31. And at episode 110 he was able to achive an average score of +30 over the last 100 consecutive episodes.\n",
    "\n",
    "OBS: The reward at each episode is the mean of the rewards of the 20 agents at the given episode (Blue Line)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed7d46ff",
   "metadata": {},
   "source": [
    "![alt text](assets/scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ba4a52",
   "metadata": {},
   "source": [
    "### Ideas for Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa327c3",
   "metadata": {},
   "source": [
    "There are some imporvements that may be worth to be done in order to make the agent to solve the enviroment faster.\n",
    "1. Try to implement other algorithms like Trust Region Policy Optimization (TRPO)and Proximal Policy Optimization (PPO).\n",
    "2. Try to implement the Prioritized Buffer Replay."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
